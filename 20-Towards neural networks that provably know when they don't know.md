The central difference from existing OOD-methods is that we have a Bayesian framework for in-and out-distribution, where we model in-and out-distribution separately. In this framework our algorithm for training neural networks follows directly as maximum likelihood estimator which is different from the more ad-hoc methods proposed in the literature. The usage of Gaussian mixture models as the density estimator is then the essential key to get the desired provable guarantees.
