out-of-distribution data is generally hard to define a-priori, and its selection can easily bias the learning.  a concern that hyperparameters tuned with one out-of-distribution dataset might not generalize to others, the space of OoD data (ex: image pixel space) is usually too large to be covered, potentially causing a selection bias for the learning. 

the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem
