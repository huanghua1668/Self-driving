strong performance is due to the ability of the recurrent network to identify recent trends in the ego-vehicleâ€™s state, and recurrent networks are shown to perform as, well as feedforward networks with longer histories as inputs.

LSTM architecture can automatically learn relevant spatial and temporal features, reducing the need to record and input long sequences.

it is found that LSTM networks rely largely on very recent information. It is shown that, if the input to a feedforward network is expanded to incorporate state information from multiple time steps, it is able to match the performance of LSTM networks
